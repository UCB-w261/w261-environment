{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank RDD with joins\n",
    "Implementation using joins from the Spark examples repo.   \n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/pagerank_data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/pagerank_data.txt\n",
    "1 2\n",
    "1 3\n",
    "1 4\n",
    "2 1\n",
    "3 1\n",
    "4 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/test_graph.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/test_graph.txt\n",
    "2\t{'3': 1}\n",
    "3\t{'2': 2}\n",
    "4\t{'1': 1, '2': 1}\n",
    "5\t{'4': 3, '2': 1, '6': 1}\n",
    "6\t{'2': 1, '5': 2}\n",
    "7\t{'2': 1, '5': 1}\n",
    "8\t{'2': 1, '5': 1}\n",
    "9\t{'2': 1, '5': 1}\n",
    "10\t{'5': 1}\n",
    "11\t{'5': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/pagerank_data2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/pagerank_data2.txt\n",
    "1\t{'2': 1,'3': 1,'4': 1}\n",
    "2\t{'1': 1}\n",
    "3\t{'1': 1}\n",
    "4\t{'1': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank-spark-example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank-spark-example.py\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "This is an example implementation of PageRank. For more conventional use,\n",
    "Please refer to PageRank implementation provided by graphx\n",
    "Example Usage:\n",
    "bin/spark-submit examples/src/main/python/pagerank.py data/mllib/pagerank_data.txt 10\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import ast\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def quiet_logs( sc ):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "\n",
    "\n",
    "def computeContribs(row):\n",
    "    \"\"\"\n",
    "    Calculates URL contributions to the rank of other URLs.\n",
    "    input: (1, ([], 1.0))\n",
    "    \"\"\"\n",
    "    node, (edges, rank) = row\n",
    "    num_edges = len(edges)\n",
    "    \n",
    "    for edge in edges:\n",
    "        yield (edge, rank/num_edges )\n",
    "        \n",
    "    yield (node,0)    \n",
    "\n",
    "\n",
    "def parseNeighbors(line):\n",
    "        \"\"\"\n",
    "        Helper function to identify potential danglers and\n",
    "        write edges as a csv string for efficient aggregation.\n",
    "        input: 4\t{'1': 1, '2': 2}\n",
    "        output: 4 [1,2,2]\n",
    "        \"\"\"\n",
    "        node, edges = line.strip().split('\\t')\n",
    "        edge_string = []       \n",
    "        for edge, count in ast.literal_eval(edges).items():\n",
    "            # emit potential danglers w/ empty string\n",
    "            yield (int(edge), [])\n",
    "            # add this edge to our string of edges\n",
    "            for cnt in range(int(count)):\n",
    "                edge_string.append(int(edge))\n",
    "        # finally yield this node w/ its string formatted edge list\n",
    "        yield (int(node), edge_string)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: pagerank <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"WARN: This is a naive implementation of PageRank and is given as an example!\\n\" +\n",
    "          \"Please refer to PageRank implementation provided by graphx\",\n",
    "          file=sys.stderr)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPageRank\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    quiet_logs( spark.sparkContext )\n",
    "    \n",
    "    # Loads in input file. It should be in format of:\n",
    "    # 4\t{'1': 1, '2': 2}\n",
    "    \n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "    \n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.flatMap(lambda urls: parseNeighbors(urls)).reduceByKey(lambda a, b: a + b).cache()\n",
    "#     N=links.count()\n",
    "    print()\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "    \n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "    \n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in sorted(ranks.collect(),key=lambda x: -x[1]):\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 12:09:25 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARN: This is a naive implementation of PageRank and is given as an example!\n",
      "Please refer to PageRank implementation provided by graphx\n",
      "2019-11-15 12:09:26 INFO  SparkContext:54 - Running Spark version 2.3.1\n",
      "2019-11-15 12:09:26 INFO  SparkContext:54 - Submitted application: PythonPageRank\n",
      "2019-11-15 12:09:26 INFO  SecurityManager:54 - Changing view acls to: root\n",
      "2019-11-15 12:09:26 INFO  SecurityManager:54 - Changing modify acls to: root\n",
      "2019-11-15 12:09:26 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-11-15 12:09:26 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-11-15 12:09:26 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2019-11-15 12:09:26 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 34339.\n",
      "2019-11-15 12:09:26 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-11-15 12:09:26 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-11-15 12:09:26 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-11-15 12:09:26 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-11-15 12:09:26 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-aa9b634c-7f45-48ce-9054-720355af15ed\n",
      "2019-11-15 12:09:26 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-11-15 12:09:26 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-11-15 12:09:26 INFO  log:192 - Logging initialized @1988ms\n",
      "2019-11-15 12:09:26 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2019-11-15 12:09:26 INFO  Server:414 - Started @2051ms\n",
      "2019-11-15 12:09:26 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2019-11-15 12:09:26 INFO  AbstractConnector:278 - Started ServerConnector@6b79c53{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "2019-11-15 12:09:26 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4041.\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@31cb5daf{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e174384{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@46b67b7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c683b8a{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@48a742e0{/stages,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17aec29c{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b93ff32{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@35f359e5{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41ed5a06{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@369d60af{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24f92b99{/storage,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b6a6292{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@207c2493{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7d39b02b{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1071f9f6{/environment,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@48d76798{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@9ce8aad{/executors,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@39c6db57{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26673bd0{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@557f6323{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@289910c3{/static,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@44796a84{/,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@93371f4{/api,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2bdc0330{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3d70c21c{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:26 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://docker.w261:4041\n",
      "2019-11-15 12:09:27 INFO  SparkContext:54 - Added file file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example.py at file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example.py with timestamp 1573819767038\n",
      "2019-11-15 12:09:27 INFO  Utils:54 - Copying /media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example.py to /tmp/spark-0f52fc21-d3b2-4e0c-8324-4640cba4be74/userFiles-b004b5bc-6813-4d0b-b929-7e9d9f1ced2f/pagerank-spark-example.py\n",
      "2019-11-15 12:09:27 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-11-15 12:09:27 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42565.\n",
      "2019-11-15 12:09:27 INFO  NettyBlockTransferService:54 - Server created on docker.w261:42565\n",
      "2019-11-15 12:09:27 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-11-15 12:09:27 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, docker.w261, 42565, None)\n",
      "2019-11-15 12:09:27 INFO  BlockManagerMasterEndpoint:54 - Registering block manager docker.w261:42565 with 366.3 MB RAM, BlockManagerId(driver, docker.w261, 42565, None)\n",
      "2019-11-15 12:09:27 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, docker.w261, 42565, None)\n",
      "2019-11-15 12:09:27 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, docker.w261, 42565, None)\n",
      "2019-11-15 12:09:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@412157d0{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:27 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/spark-warehouse/').\n",
      "2019-11-15 12:09:27 INFO  SharedState:54 - Warehouse path is 'file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/spark-warehouse/'.\n",
      "2019-11-15 12:09:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b6a4982{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7b455a5e{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5605a6d4{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@70f475c2{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:27 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e43c261{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:09:27 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "2 has rank: 3.2011948302912363.\n",
      "3 has rank: 2.967951701532742.\n",
      "5 has rank: 0.7538731095706107.\n",
      "4 has rank: 0.5344752869428572.\n",
      "1 has rank: 0.37715199653387405.\n",
      "6 has rank: 0.2781584289809524.\n",
      "7 has rank: 0.15.\n",
      "8 has rank: 0.15.\n",
      "9 has rank: 0.15.\n",
      "10 has rank: 0.15.\n",
      "11 has rank: 0.15.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit pagerank-spark-example.py data/test_graph.txt 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(2, 0.3620640495978871),\n",
    " (3, 0.333992700474142),\n",
    " (5, 0.08506399429624555),\n",
    " (4, 0.06030963508473455),\n",
    " (1, 0.04255740809817991),\n",
    " (6, 0.03138662354831139),\n",
    " (8, 0.01692511778009981),\n",
    " (10, 0.01692511778009981),\n",
    " (7, 0.01692511778009981),\n",
    " (9, 0.01692511778009981),\n",
    " (11, 0.01692511778009981)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 12:08:08 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARN: This is a naive implementation of PageRank and is given as an example!\n",
      "Please refer to PageRank implementation provided by graphx\n",
      "2019-11-15 12:08:08 INFO  SparkContext:54 - Running Spark version 2.3.1\n",
      "2019-11-15 12:08:08 INFO  SparkContext:54 - Submitted application: PythonPageRank\n",
      "2019-11-15 12:08:09 INFO  SecurityManager:54 - Changing view acls to: root\n",
      "2019-11-15 12:08:09 INFO  SecurityManager:54 - Changing modify acls to: root\n",
      "2019-11-15 12:08:09 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-11-15 12:08:09 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-11-15 12:08:09 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2019-11-15 12:08:09 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 41503.\n",
      "2019-11-15 12:08:09 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-11-15 12:08:09 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-11-15 12:08:09 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-11-15 12:08:09 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-11-15 12:08:09 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-4666cb40-7504-414f-8722-9bc91acd235c\n",
      "2019-11-15 12:08:09 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-11-15 12:08:09 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-11-15 12:08:09 INFO  log:192 - Logging initialized @1954ms\n",
      "2019-11-15 12:08:09 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2019-11-15 12:08:09 INFO  Server:414 - Started @2019ms\n",
      "2019-11-15 12:08:09 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2019-11-15 12:08:09 INFO  AbstractConnector:278 - Started ServerConnector@e003c5f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "2019-11-15 12:08:09 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4041.\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3feb6194{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4feb4933{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2816b3cc{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37c18968{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2276ec72{/stages,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3fe3a323{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7358f690{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f31816c{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4be9046c{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4edf5530{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5dff7cb1{/storage,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2eb6bc83{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2dff075d{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@19bda107{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3f814c75{/environment,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24c8bda2{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71d89389{/executors,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@599a12c{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@304ea70e{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@510a2908{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@647bbc7b{/static,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@35fec7b7{/,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ee37d78{/api,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b95b54e{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d087c8b{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:09 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://docker.w261:4041\n",
      "2019-11-15 12:08:09 INFO  SparkContext:54 - Added file file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example.py at file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example.py with timestamp 1573819689841\n",
      "2019-11-15 12:08:09 INFO  Utils:54 - Copying /media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example.py to /tmp/spark-d2312420-6abe-4582-a1ae-0467a0ea0ef5/userFiles-b364d96f-0877-443e-9222-b4a423a82be7/pagerank-spark-example.py\n",
      "2019-11-15 12:08:09 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-11-15 12:08:09 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45679.\n",
      "2019-11-15 12:08:09 INFO  NettyBlockTransferService:54 - Server created on docker.w261:45679\n",
      "2019-11-15 12:08:09 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-11-15 12:08:09 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, docker.w261, 45679, None)\n",
      "2019-11-15 12:08:09 INFO  BlockManagerMasterEndpoint:54 - Registering block manager docker.w261:45679 with 366.3 MB RAM, BlockManagerId(driver, docker.w261, 45679, None)\n",
      "2019-11-15 12:08:09 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, docker.w261, 45679, None)\n",
      "2019-11-15 12:08:09 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, docker.w261, 45679, None)\n",
      "2019-11-15 12:08:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@138043e6{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:10 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/spark-warehouse/').\n",
      "2019-11-15 12:08:10 INFO  SharedState:54 - Warehouse path is 'file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/spark-warehouse/'.\n",
      "2019-11-15 12:08:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3cd69dc0{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@84ee514{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a8ba5d7{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@223bd6ec{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@530618ee{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:08:10 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "[(2, [1]), (3, [1]), (4, [1]), (1, [2, 3, 4])]\n",
      "1 has rank: 1.7380073041193354.\n",
      "2 has rank: 0.7539975652935547.\n",
      "3 has rank: 0.7539975652935547.\n",
      "4 has rank: 0.7539975652935547.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit pagerank-spark-example.py data/pagerank_data2.txt 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9999999999999996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 has rank: 0.7539975652935547.\n",
    "3 has rank: 0.7539975652935547.\n",
    "1 has rank: 1.7380073041193354.\n",
    "4 has rank: 0.7539975652935547."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank-spark-example-org.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank-spark-example-org.py\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "This is an example implementation of PageRank. For more conventional use,\n",
    "Please refer to PageRank implementation provided by graphx\n",
    "Example Usage:\n",
    "bin/spark-submit examples/src/main/python/pagerank.py data/mllib/pagerank_data.txt 10\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def quiet_logs( sc ):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\"). setLevel( logger.Level.ERROR )\n",
    "    logger.LogManager.getLogger(\"akka\").setLevel( logger.Level.ERROR )\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: pagerank <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"WARN: This is a naive implementation of PageRank and is given as an example!\\n\" +\n",
    "          \"Please refer to PageRank implementation provided by graphx\",\n",
    "          file=sys.stderr)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPageRank\")\\\n",
    "        .getOrCreate()\n",
    "    quiet_logs( spark.sparkContext )\n",
    "\n",
    "    # Loads in input file. It should be in format of:\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     ...\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "    for l in links.collect():\n",
    "        print(l[0],list(l[1]))\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in ranks.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 12:05:21 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "WARN: This is a naive implementation of PageRank and is given as an example!\n",
      "Please refer to PageRank implementation provided by graphx\n",
      "2019-11-15 12:05:22 INFO  SparkContext:54 - Running Spark version 2.3.1\n",
      "2019-11-15 12:05:22 INFO  SparkContext:54 - Submitted application: PythonPageRank\n",
      "2019-11-15 12:05:22 INFO  SecurityManager:54 - Changing view acls to: root\n",
      "2019-11-15 12:05:22 INFO  SecurityManager:54 - Changing modify acls to: root\n",
      "2019-11-15 12:05:22 INFO  SecurityManager:54 - Changing view acls groups to: \n",
      "2019-11-15 12:05:22 INFO  SecurityManager:54 - Changing modify acls groups to: \n",
      "2019-11-15 12:05:22 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "2019-11-15 12:05:22 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 40141.\n",
      "2019-11-15 12:05:22 INFO  SparkEnv:54 - Registering MapOutputTracker\n",
      "2019-11-15 12:05:22 INFO  SparkEnv:54 - Registering BlockManagerMaster\n",
      "2019-11-15 12:05:22 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2019-11-15 12:05:22 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\n",
      "2019-11-15 12:05:22 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-1b30fbbf-e379-48db-aba6-33e0f05a06db\n",
      "2019-11-15 12:05:22 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB\n",
      "2019-11-15 12:05:22 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\n",
      "2019-11-15 12:05:22 INFO  log:192 - Logging initialized @2015ms\n",
      "2019-11-15 12:05:22 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\n",
      "2019-11-15 12:05:22 INFO  Server:414 - Started @2103ms\n",
      "2019-11-15 12:05:22 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2019-11-15 12:05:22 INFO  AbstractConnector:278 - Started ServerConnector@7a64f389{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
      "2019-11-15 12:05:22 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4041.\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@79da0eb3{/jobs,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@663c6994{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30c73419{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1f3dd8f9{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1a487243{/stages,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1326faef{/stages/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6be0ad49{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@673d82b8{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@959f1f3{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4924f7be{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d85721b{/storage,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@46a60151{/storage/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b6a61f{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6ec138b0{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@8329ad9{/environment,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e68be07{/environment/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b5b798b{/executors,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2cb8cce8{/executors/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30c06ff4{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@be3776d{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1be4d0bb{/static,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@27252ba2{/,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@dd2f7c6{/api,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@57c4a607{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@19182113{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:22 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://docker.w261:4041\n",
      "2019-11-15 12:05:23 INFO  SparkContext:54 - Added file file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example-org.py at file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example-org.py with timestamp 1573819523103\n",
      "2019-11-15 12:05:23 INFO  Utils:54 - Copying /media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/pagerank-spark-example-org.py to /tmp/spark-5dc29732-5aef-4e8a-9752-d98c424a6084/userFiles-c2e760ca-1ab2-4ed7-8fbc-8c4863da4390/pagerank-spark-example-org.py\n",
      "2019-11-15 12:05:23 INFO  Executor:54 - Starting executor ID driver on host localhost\n",
      "2019-11-15 12:05:23 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34515.\n",
      "2019-11-15 12:05:23 INFO  NettyBlockTransferService:54 - Server created on docker.w261:34515\n",
      "2019-11-15 12:05:23 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2019-11-15 12:05:23 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, docker.w261, 34515, None)\n",
      "2019-11-15 12:05:23 INFO  BlockManagerMasterEndpoint:54 - Registering block manager docker.w261:34515 with 366.3 MB RAM, BlockManagerId(driver, docker.w261, 34515, None)\n",
      "2019-11-15 12:05:23 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, docker.w261, 34515, None)\n",
      "2019-11-15 12:05:23 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, docker.w261, 34515, None)\n",
      "2019-11-15 12:05:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5db8556c{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:23 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/spark-warehouse/').\n",
      "2019-11-15 12:05:23 INFO  SharedState:54 - Warehouse path is 'file:/media/notebooks/LiveSessionMaterials/wk10Demo_PageRank/master/spark-warehouse/'.\n",
      "2019-11-15 12:05:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@78c9b562{/SQL,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ef960fc{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@38b71ed2{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59ebae6{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:23 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@39c445d8{/static/sql,null,AVAILABLE,@Spark}\n",
      "2019-11-15 12:05:24 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\n",
      "1 ['2', '3', '4']\n",
      "2 ['1']\n",
      "3 ['1']\n",
      "4 ['1']\n",
      "2 has rank: 0.7539975652935547.\n",
      "3 has rank: 0.7539975652935547.\n",
      "1 has rank: 1.7380073041193354.\n",
      "4 has rank: 0.7539975652935547.\n"
     ]
    }
   ],
   "source": [
    "!spark-submit pagerank-spark-example-org.py data/pagerank_data.txt 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
